{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our class names: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Processing speaker blues\n",
      "Processing speaker classical\n",
      "Processing speaker country\n",
      "Processing speaker disco\n",
      "Processing speaker hiphop\n",
      "Processing speaker jazz\n",
      "Processing speaker metal\n",
      "Processing speaker pop\n",
      "Processing speaker reggae\n",
      "Processing speaker rock\n",
      "Found 1000 files belonging to 10 classes.\n",
      "Using 800 files for training.\n",
      "Using 200 files for validation and test.\n",
      "Using 100 files for test.\n",
      "Using 100 files for validation.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000286943CAB70> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000286943CAB70> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000286943D8B70> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000286943D8B70> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000286943D8C80> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000286943D8C80> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dividing to test, validation and training set\n",
    "DATASET_AUDIO_PATH = \"Data/genres_original/\"\n",
    "\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "# Seed to use when shuffling the dataset and the noise\n",
    "SHUFFLE_SEED = 43\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_and_TEST_SPLIT = 0.2\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_SPLIT = 0.5\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 25\n",
    "SAMPLING_RATE = 20000\n",
    "\n",
    "audio_paths = [] # list of paths for every audio sample\n",
    "labels = [] # classification\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
    ")\n",
    "\n",
    "\n",
    "# Shuffle for equals splitting\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# 80:10:10\n",
    "# Split into training, validation and test\n",
    "num_val_samples = int(VALID_and_TEST_SPLIT * len(audio_paths))\n",
    "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "print(\"Using {} files for validation and test.\".format(num_val_samples))\n",
    "valid_test_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_test_labels = labels[-num_val_samples:]\n",
    "\n",
    "num_test_samples = int(VALID_SPLIT * len(valid_test_audio_paths))\n",
    "print(\"Using {} files for test.\".format(num_test_samples))\n",
    "test_audio_paths = audio_paths[-num_test_samples:]\n",
    "test_labels = labels[-num_test_samples:]\n",
    "\n",
    "print(\"Using {} files for validation.\".format(num_test_samples))\n",
    "valid_audio_paths = audio_paths[:-num_test_samples]\n",
    "valid_labels = labels[:-num_test_samples]\n",
    "\n",
    "\n",
    "# Create 3 datasets, one for training, one for validation and the last for test\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "test_ds = paths_and_labels_to_dataset(test_audio_paths, test_labels)\n",
    "test_ds = test_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "####\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 10000, 1)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 10000, 16)    64          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 10000, 16)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 10000, 16)    784         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 10000, 16)    32          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 10000, 16)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10000, 16)    0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 5000, 16)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 5000, 32)     1568        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 5000, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 5000, 32)     3104        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 5000, 32)     544         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 5000, 32)     0           conv1d_5[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 5000, 32)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2500, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2500, 64)     6208        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 2500, 64)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2500, 64)     12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 2500, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2500, 64)     12352       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2500, 64)     2112        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2500, 64)     0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2500, 64)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1250, 64)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1250, 128)    24704       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1250, 128)    0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1250, 128)    49280       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1250, 128)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1250, 128)    49280       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1250, 128)    8320        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1250, 128)    0           conv1d_13[0][0]                  \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1250, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 625, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 625, 128)     49280       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 625, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 625, 128)     49280       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 625, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 625, 128)     49280       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 625, 128)     16512       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 625, 128)     0           conv1d_17[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 625, 128)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 312, 128)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d (AveragePooli (None, 104, 128)     0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 13312)        0           average_pooling1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          3408128     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           1290        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,777,370\n",
      "Trainable params: 3,777,370\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add callbacks:\n",
    "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
    "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
    "model_save_filename = \"model.h5\"\n",
    "\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps, validate for 29 steps\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 87s 12s/step - loss: 4.5528 - accuracy: 0.1275 - val_loss: 2.2809 - val_accuracy: 0.2011\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 79s 11s/step - loss: 2.1541 - accuracy: 0.2575 - val_loss: 1.9476 - val_accuracy: 0.3178\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 84s 12s/step - loss: 1.8088 - accuracy: 0.3575 - val_loss: 1.7460 - val_accuracy: 0.4122\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 83s 12s/step - loss: 1.6505 - accuracy: 0.4050 - val_loss: 1.5756 - val_accuracy: 0.4844\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 83s 12s/step - loss: 1.4949 - accuracy: 0.5025 - val_loss: 1.3434 - val_accuracy: 0.5178\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 85s 12s/step - loss: 1.3292 - accuracy: 0.5525 - val_loss: 1.3526 - val_accuracy: 0.5267\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 1.2773 - accuracy: 0.5700 - val_loss: 1.1493 - val_accuracy: 0.6311\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 59s 8s/step - loss: 1.1129 - accuracy: 0.6263 - val_loss: 1.3303 - val_accuracy: 0.5622\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 67s 10s/step - loss: 1.1045 - accuracy: 0.6225 - val_loss: 1.0167 - val_accuracy: 0.6544\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 59s 8s/step - loss: 0.9330 - accuracy: 0.6837 - val_loss: 0.8741 - val_accuracy: 0.7333\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.8146 - accuracy: 0.7225 - val_loss: 0.8249 - val_accuracy: 0.7156\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 59s 8s/step - loss: 0.7301 - accuracy: 0.7487 - val_loss: 0.8636 - val_accuracy: 0.7467\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.6562 - accuracy: 0.7850 - val_loss: 0.7723 - val_accuracy: 0.7678\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.5703 - accuracy: 0.8025 - val_loss: 0.7504 - val_accuracy: 0.7933\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.5324 - accuracy: 0.8400 - val_loss: 0.5961 - val_accuracy: 0.8244\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.3748 - accuracy: 0.8725 - val_loss: 0.8746 - val_accuracy: 0.7867\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.3825 - accuracy: 0.8637 - val_loss: 0.5078 - val_accuracy: 0.8767\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.2875 - accuracy: 0.8963 - val_loss: 0.5526 - val_accuracy: 0.8744\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 61s 9s/step - loss: 0.1920 - accuracy: 0.9413 - val_loss: 0.5066 - val_accuracy: 0.8978\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.1597 - accuracy: 0.9463 - val_loss: 0.5677 - val_accuracy: 0.9000\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.2916 - accuracy: 0.9112 - val_loss: 0.5849 - val_accuracy: 0.8867\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 0.2378 - accuracy: 0.9300 - val_loss: 0.7016 - val_accuracy: 0.8178\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.2688 - accuracy: 0.9062 - val_loss: 0.6346 - val_accuracy: 0.8656\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 59s 8s/step - loss: 0.2462 - accuracy: 0.9100 - val_loss: 0.5539 - val_accuracy: 0.8833\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 0.1066 - accuracy: 0.9625 - val_loss: 0.6400 - val_accuracy: 0.9144\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.0651 - accuracy: 0.9775 - val_loss: 0.5589 - val_accuracy: 0.9344\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.0437 - accuracy: 0.9912 - val_loss: 0.6067 - val_accuracy: 0.9333\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 59s 8s/step - loss: 0.0601 - accuracy: 0.9912 - val_loss: 0.6338 - val_accuracy: 0.9267\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.1032 - accuracy: 0.9700 - val_loss: 0.9246 - val_accuracy: 0.8878\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
