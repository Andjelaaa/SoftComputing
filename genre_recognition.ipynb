{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Korisnik\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our class names: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Processing speaker blues\n",
      "Processing speaker classical\n",
      "Processing speaker country\n",
      "Processing speaker disco\n",
      "Processing speaker hiphop\n",
      "Processing speaker jazz\n",
      "Processing speaker metal\n",
      "Processing speaker pop\n",
      "Processing speaker reggae\n",
      "Processing speaker rock\n",
      "Found 1000 files belonging to 10 classes.\n",
      "Using 800 files for training.\n",
      "Using 200 files for validation and test.\n",
      "Using 100 files for test.\n",
      "Using 100 files for validation.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000226C90D8AE8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000226C90D8AE8> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000226C90D8510> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000226C90D8510> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000226C913C488> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000226C913C488> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dividing to test, validation and training set\n",
    "DATASET_AUDIO_PATH = \"Data/genres_original/\"\n",
    "\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "# Seed to use when shuffling the dataset and the noise\n",
    "SHUFFLE_SEED = 43\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_and_TEST_SPLIT = 0.2\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_SPLIT = 0.5\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "audio_paths = [] # list of paths for every audio sample\n",
    "labels = [] # classification\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
    ")\n",
    "\n",
    "\n",
    "# Shuffle for equals splitting\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# 80:10:10\n",
    "# Split into training, validation and test\n",
    "num_val_samples = int(VALID_and_TEST_SPLIT * len(audio_paths))\n",
    "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "print(\"Using {} files for validation and test.\".format(num_val_samples))\n",
    "valid_test_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_test_labels = labels[-num_val_samples:]\n",
    "\n",
    "num_test_samples = int(VALID_SPLIT * len(valid_test_audio_paths))\n",
    "print(\"Using {} files for test.\".format(num_test_samples))\n",
    "test_audio_paths = audio_paths[-num_test_samples:]\n",
    "test_labels = labels[-num_test_samples:]\n",
    "\n",
    "print(\"Using {} files for validation.\".format(num_test_samples))\n",
    "valid_audio_paths = audio_paths[:-num_test_samples]\n",
    "valid_labels = labels[:-num_test_samples]\n",
    "\n",
    "\n",
    "# Create 3 datasets, one for training, one for validation and the last for test\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "test_ds = paths_and_labels_to_dataset(test_audio_paths, test_labels)\n",
    "test_ds = test_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "####\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 8000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 8000, 16)     64          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 8000, 16)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 8000, 16)     784         activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 8000, 16)     32          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8000, 16)     0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8000, 16)     0           add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 4000, 16)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 4000, 32)     1568        max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 4000, 32)     0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 4000, 32)     3104        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 4000, 32)     544         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 4000, 32)     0           conv1d_5[0][0]                   \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 4000, 32)     0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 2000, 32)     0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2000, 64)     6208        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 2000, 64)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 2000, 64)     12352       activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 2000, 64)     0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 2000, 64)     12352       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2000, 64)     2112        max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 2000, 64)     0           conv1d_9[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 2000, 64)     0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1000, 64)     0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 1000, 128)    24704       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 1000, 128)    0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 1000, 128)    49280       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 1000, 128)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 1000, 128)    49280       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 1000, 128)    8320        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 1000, 128)    0           conv1d_13[0][0]                  \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 1000, 128)    0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 500, 128)     0           activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 500, 128)     49280       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 500, 128)     0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 500, 128)     49280       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 500, 128)     0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 500, 128)     49280       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 500, 128)     16512       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 500, 128)     0           conv1d_17[0][0]                  \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 500, 128)     0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 250, 128)     0           activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d (AveragePooli (None, 83, 128)      0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10624)        0           average_pooling1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          2720000     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          32896       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           1290        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,089,242\n",
      "Trainable params: 3,089,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add callbacks:\n",
    "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
    "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
    "model_save_filename = \"model.h5\"\n",
    "\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps, validate for 29 steps\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 3.7362 - accuracy: 0.1375 - val_loss: 2.1684 - val_accuracy: 0.2411\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 56s 8s/step - loss: 2.1032 - accuracy: 0.2537 - val_loss: 1.9309 - val_accuracy: 0.3878\n",
      "Epoch 3/100\n",
      "4/7 [================>.............] - ETA: 20s - loss: 1.8797 - accuracy: 0.3887"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
