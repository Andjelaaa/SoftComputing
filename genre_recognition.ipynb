{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "import librosa\n",
    "import librosa.display\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our class names: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "Processing speaker blues\n",
      "Processing speaker classical\n",
      "Processing speaker country\n",
      "Processing speaker disco\n",
      "Processing speaker hiphop\n",
      "Processing speaker jazz\n",
      "Processing speaker metal\n",
      "Processing speaker pop\n",
      "Processing speaker reggae\n",
      "Processing speaker rock\n",
      "Found 1000 files belonging to 10 classes.\n",
      "Using 800 files for training.\n",
      "Using 200 files for validation and test.\n",
      "Using 100 files for test.\n",
      "Using 100 files for validation.\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000286943D8C80> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000286943D8C80> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x00000286943CA0D0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x00000286943CA0D0> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x0000028690EA1A60> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n",
      "WARNING: AutoGraph could not transform <function <lambda> at 0x0000028690EA1A60> and will run it as-is.\n",
      "Cause: could not parse the source code:\n",
      "\n",
      "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
      "\n",
      "This error may be avoided by creating the lambda in a standalone statement.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dividing to test, validation and training set\n",
    "DATASET_AUDIO_PATH = \"Data/genres_original/\"\n",
    "\n",
    "def paths_and_labels_to_dataset(audio_paths, labels):\n",
    "    \"\"\"Constructs a dataset of audios and labels.\"\"\"\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: path_to_audio(x))\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    \"\"\"Reads and decodes an audio file.\"\"\"\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, SAMPLING_RATE)\n",
    "    return audio\n",
    "\n",
    "\n",
    "class_names = os.listdir(DATASET_AUDIO_PATH)\n",
    "print(\"Our class names: {}\".format(class_names,))\n",
    "\n",
    "# Seed to use when shuffling the dataset and the noise\n",
    "SHUFFLE_SEED = 43\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_and_TEST_SPLIT = 0.2\n",
    "\n",
    "# Percentage of samples to use for validation and test\n",
    "VALID_SPLIT = 0.5\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 100\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "audio_paths = [] # list of paths for every audio sample\n",
    "labels = [] # classification\n",
    "for label, name in enumerate(class_names):\n",
    "    print(\"Processing speaker {}\".format(name,))\n",
    "    dir_path = Path(DATASET_AUDIO_PATH) / name\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "print(\n",
    "    \"Found {} files belonging to {} classes.\".format(len(audio_paths), len(class_names))\n",
    ")\n",
    "\n",
    "\n",
    "# Shuffle for equals splitting\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(audio_paths)\n",
    "rng = np.random.RandomState(SHUFFLE_SEED)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# 80:10:10\n",
    "# Split into training, validation and test\n",
    "num_val_samples = int(VALID_and_TEST_SPLIT * len(audio_paths))\n",
    "print(\"Using {} files for training.\".format(len(audio_paths) - num_val_samples))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "\n",
    "print(\"Using {} files for validation and test.\".format(num_val_samples))\n",
    "valid_test_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_test_labels = labels[-num_val_samples:]\n",
    "\n",
    "num_test_samples = int(VALID_SPLIT * len(valid_test_audio_paths))\n",
    "print(\"Using {} files for test.\".format(num_test_samples))\n",
    "test_audio_paths = audio_paths[-num_test_samples:]\n",
    "test_labels = labels[-num_test_samples:]\n",
    "\n",
    "print(\"Using {} files for validation.\".format(num_test_samples))\n",
    "valid_audio_paths = audio_paths[:-num_test_samples]\n",
    "valid_labels = labels[:-num_test_samples]\n",
    "\n",
    "\n",
    "# Create 3 datasets, one for training, one for validation and the last for test\n",
    "train_ds = paths_and_labels_to_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=BATCH_SIZE * 8, seed=SHUFFLE_SEED).batch(\n",
    "    BATCH_SIZE\n",
    ")\n",
    "valid_ds = paths_and_labels_to_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "test_ds = paths_and_labels_to_dataset(test_audio_paths, test_labels)\n",
    "test_ds = test_ds.shuffle(buffer_size=32 * 8, seed=SHUFFLE_SEED).batch(32)\n",
    "\n",
    "####\n",
    "def audio_to_fft(audio):\n",
    "    # Since tf.signal.fft applies FFT on the innermost dimension,\n",
    "    # we need to squeeze the dimensions and then expand them again\n",
    "    # after FFT\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(\n",
    "        tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64)\n",
    "    )\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "\n",
    "    # Return the absolute value of the first half of the FFT\n",
    "    # which represents the positive frequencies\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "# Transform audio wave to the frequency domain using `audio_to_fft`\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "valid_ds = valid_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "valid_ds = valid_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_ds = test_ds.map(\n",
    "    lambda x, y: (audio_to_fft(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE\n",
    ")\n",
    "test_ds = test_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def residual_block(x, filters, conv_num=3, activation=\"relu\"):\n",
    "    # Shortcut\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "\n",
    "    x = residual_block(inputs, 16, 2)\n",
    "    x = residual_block(x, 32, 2)\n",
    "    x = residual_block(x, 64, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "    x = residual_block(x, 128, 3)\n",
    "\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "model = build_model((SAMPLING_RATE // 2, 1), len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 8000, 1)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 8000, 16)     64          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 8000, 16)     0           conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, 8000, 16)     784         activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 8000, 16)     32          input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8000, 16)     0           conv1d_20[0][0]                  \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 8000, 16)     0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 4000, 16)     0           activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, 4000, 32)     1568        max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 4000, 32)     0           conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 4000, 32)     3104        activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, 4000, 32)     544         max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 4000, 32)     0           conv1d_23[0][0]                  \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 4000, 32)     0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 2000, 32)     0           activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 2000, 64)     6208        max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 2000, 64)     0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 2000, 64)     12352       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 2000, 64)     0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, 2000, 64)     12352       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 2000, 64)     2112        max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 2000, 64)     0           conv1d_27[0][0]                  \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 2000, 64)     0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1000, 64)     0           activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, 1000, 128)    24704       max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 1000, 128)    0           conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, 1000, 128)    49280       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 1000, 128)    0           conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, 1000, 128)    49280       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, 1000, 128)    8320        max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 1000, 128)    0           conv1d_31[0][0]                  \n",
      "                                                                 conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 1000, 128)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 500, 128)     0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, 500, 128)     49280       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 500, 128)     0           conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 500, 128)     49280       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 500, 128)     0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 500, 128)     49280       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, 500, 128)     16512       max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 500, 128)     0           conv1d_35[0][0]                  \n",
      "                                                                 conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 500, 128)     0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 250, 128)     0           activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling1d_1 (AveragePoo (None, 83, 128)      0           max_pooling1d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 10624)        0           average_pooling1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 256)          2720000     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          32896       dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 10)           1290        dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,089,242\n",
      "Trainable params: 3,089,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model using Adam's default learning rate\n",
    "model.compile(\n",
    "    optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add callbacks:\n",
    "# 'EarlyStopping' to stop training when the model is not enhancing anymore\n",
    "# 'ModelCheckPoint' to always keep the model that has the best val_accuracy\n",
    "model_save_filename = \"model.h5\"\n",
    "\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    model_save_filename, monitor=\"val_accuracy\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 7 steps, validate for 29 steps\n",
      "Epoch 1/100\n",
      "7/7 [==============================] - 69s 10s/step - loss: 3.7585 - accuracy: 0.1187 - val_loss: 2.2527 - val_accuracy: 0.1356\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 2.1716 - accuracy: 0.2125 - val_loss: 2.0019 - val_accuracy: 0.2456\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 58s 8s/step - loss: 1.8748 - accuracy: 0.3100 - val_loss: 1.7561 - val_accuracy: 0.3656\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 1.6835 - accuracy: 0.4087 - val_loss: 1.6702 - val_accuracy: 0.4411\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 1.6013 - accuracy: 0.4563 - val_loss: 1.4493 - val_accuracy: 0.4944\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 1.4484 - accuracy: 0.5038 - val_loss: 1.4373 - val_accuracy: 0.5222\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 70s 10s/step - loss: 1.3913 - accuracy: 0.5300 - val_loss: 1.3651 - val_accuracy: 0.5289\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 1.2580 - accuracy: 0.5512 - val_loss: 1.3058 - val_accuracy: 0.5678\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 1.1916 - accuracy: 0.5838 - val_loss: 1.1556 - val_accuracy: 0.6144\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 69s 10s/step - loss: 1.0979 - accuracy: 0.6363 - val_loss: 1.1749 - val_accuracy: 0.6100\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 67s 10s/step - loss: 0.9731 - accuracy: 0.6725 - val_loss: 0.9348 - val_accuracy: 0.6944\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 73s 10s/step - loss: 0.8469 - accuracy: 0.7150 - val_loss: 0.8893 - val_accuracy: 0.7244\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.7510 - accuracy: 0.7663 - val_loss: 0.7423 - val_accuracy: 0.7889\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 0.5649 - accuracy: 0.8213 - val_loss: 0.9750 - val_accuracy: 0.7256\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 72s 10s/step - loss: 0.6043 - accuracy: 0.7975 - val_loss: 0.8954 - val_accuracy: 0.7422\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.7237 - accuracy: 0.7638 - val_loss: 1.0466 - val_accuracy: 0.7044\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.5782 - accuracy: 0.7900 - val_loss: 0.6758 - val_accuracy: 0.7856\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.3696 - accuracy: 0.8788 - val_loss: 0.6862 - val_accuracy: 0.8489\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.3089 - accuracy: 0.8813 - val_loss: 0.4918 - val_accuracy: 0.8944\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.2209 - accuracy: 0.9262 - val_loss: 0.5800 - val_accuracy: 0.8600\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 64s 9s/step - loss: 0.2225 - accuracy: 0.9250 - val_loss: 0.6813 - val_accuracy: 0.8567\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 69s 10s/step - loss: 0.2789 - accuracy: 0.9100 - val_loss: 0.6181 - val_accuracy: 0.8544\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 69s 10s/step - loss: 0.4350 - accuracy: 0.8712 - val_loss: 0.7901 - val_accuracy: 0.7967\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.3170 - accuracy: 0.9050 - val_loss: 0.4476 - val_accuracy: 0.8978\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.1529 - accuracy: 0.9525 - val_loss: 0.6040 - val_accuracy: 0.9089\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 63s 9s/step - loss: 0.1048 - accuracy: 0.9650 - val_loss: 0.4883 - val_accuracy: 0.9333\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.0624 - accuracy: 0.9800 - val_loss: 0.6039 - val_accuracy: 0.9344\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 62s 9s/step - loss: 0.0455 - accuracy: 0.9912 - val_loss: 0.6659 - val_accuracy: 0.9300\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 60s 9s/step - loss: 0.1670 - accuracy: 0.9475 - val_loss: 0.7755 - val_accuracy: 0.8844\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 57s 8s/step - loss: 0.2175 - accuracy: 0.9325 - val_loss: 0.5151 - val_accuracy: 0.9022\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 66s 9s/step - loss: 0.1718 - accuracy: 0.9450 - val_loss: 0.5729 - val_accuracy: 0.9000\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.1102 - accuracy: 0.9663 - val_loss: 0.4153 - val_accuracy: 0.9356\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.0539 - accuracy: 0.9900 - val_loss: 0.4213 - val_accuracy: 0.9433\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.0362 - accuracy: 0.9950 - val_loss: 0.6750 - val_accuracy: 0.9389\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.0557 - accuracy: 0.9837 - val_loss: 0.6683 - val_accuracy: 0.9367\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.0582 - accuracy: 0.9825 - val_loss: 0.5526 - val_accuracy: 0.9278\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.0323 - accuracy: 0.9925 - val_loss: 0.4725 - val_accuracy: 0.9378\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.0379 - accuracy: 0.9900 - val_loss: 0.4442 - val_accuracy: 0.9456\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 54s 8s/step - loss: 0.0439 - accuracy: 0.9862 - val_loss: 0.5050 - val_accuracy: 0.9456\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 56s 8s/step - loss: 0.0535 - accuracy: 0.9900 - val_loss: 0.5928 - val_accuracy: 0.9267\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.0715 - accuracy: 0.9837 - val_loss: 0.5434 - val_accuracy: 0.9378\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 55s 8s/step - loss: 0.1873 - accuracy: 0.9450 - val_loss: 0.5474 - val_accuracy: 0.9267\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
